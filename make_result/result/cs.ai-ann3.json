{
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_48": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_48",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_48",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_48",
		"text": "The synthetic motherset was generated by producing 10,000 candidate normals and 10,000 candidate anomalies from two different multivariate distributions with the intention of being able to manipulate all problem dimensions with ease . The candidate normals are drawn from a multivariate gaussian with a covariance matrix of $I$ ; that is , each feature is drawn from the standard normal distribution independently of the others . The anomalies are drawn uniformly from the hyper-cube defined by the range $(-4,4)$ in each dimension . Both distributions have ten dimensions ; that is , each point exists in $R^{10}$ .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 325,
				"end": 326,
				"text": "I"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 506,
				"end": 512,
				"text": "(-4,4)"
			},
			"T3": {
				"eid": "T3",
				"label": "PRIMARY",
				"start": 303,
				"end": 320,
				"text": "covariance matrix"
			},
			"T4": {
				"eid": "T4",
				"label": "PRIMARY",
				"start": 495,
				"end": 504,
				"text": "the range"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 499,
				"end": 504,
				"text": "range"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T3",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T4",
				"arg1": "T2",
				"rid": "R2"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R3"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_49": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_49",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_49",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_49",
		"text": "Our heuristic procedure begins by training a Random Forest as in \\cite{Breiman} to solve the multi-class classification problem . Then we calculate the amount of confusion between each pair of classes . For each data point $x_i$ , the Random Forest computes an estimate of $P(\\hat{y_i}=k|x_i)$ , the predicted probability that $x_i$ belongs to class $k$ . We construct a confusion matrix $C$ in which cell $C_{j,k}$ contains the sum of $P(\\hat{y_i}=k|x_i)$ for all $x_i$ whose true class $y_i = j$ . We then define a graph in which each node is a class and each edge ( between two classes $j$ and $k$ ) has a weight equal to $C[j,k]+C[k,j]$ . This is the ( un-normalized ) probability that a data point in class $j$ will be confused with a data point in class $k$ or vice versa . We then compute the maximum weight spanning tree of this ( complete ) graph to identify a graph of `` most - confusable ' ' relationships between pairs of classes . We then two - color this tree so that no adjacent nodes have the same color . The two colors define the two classes of points .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 224,
				"end": 227,
				"text": "x_i"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 274,
				"end": 292,
				"text": "P(\\hat{y_i}=k|x_i)"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 288,
				"end": 291,
				"text": "x_i"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 328,
				"end": 331,
				"text": "x_i"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 351,
				"end": 352,
				"text": "k"
			},
			"T6": {
				"eid": "T6",
				"label": "SYMBOL",
				"start": 389,
				"end": 390,
				"text": "C"
			},
			"T7": {
				"eid": "T7",
				"label": "SYMBOL",
				"start": 407,
				"end": 414,
				"text": "C_{j,k}"
			},
			"T8": {
				"eid": "T8",
				"label": "SYMBOL",
				"start": 466,
				"end": 469,
				"text": "x_i"
			},
			"T9": {
				"eid": "T9",
				"label": "SYMBOL",
				"start": 489,
				"end": 492,
				"text": "y_i"
			},
			"T10": {
				"eid": "T10",
				"label": "SYMBOL",
				"start": 495,
				"end": 496,
				"text": "j"
			},
			"T11": {
				"eid": "T11",
				"label": "SYMBOL",
				"start": 590,
				"end": 591,
				"text": "j"
			},
			"T12": {
				"eid": "T12",
				"label": "SYMBOL",
				"start": 598,
				"end": 599,
				"text": "k"
			},
			"T13": {
				"eid": "T13",
				"label": "SYMBOL",
				"start": 626,
				"end": 632,
				"text": "C[j,k]"
			},
			"T14": {
				"eid": "T14",
				"label": "SYMBOL",
				"start": 626,
				"end": 639,
				"text": "C[j,k]+C[k,j]"
			},
			"T15": {
				"eid": "T15",
				"label": "SYMBOL",
				"start": 630,
				"end": 631,
				"text": "k"
			},
			"T16": {
				"eid": "T16",
				"label": "SYMBOL",
				"start": 637,
				"end": 638,
				"text": "j"
			},
			"T17": {
				"eid": "T17",
				"label": "SYMBOL",
				"start": 713,
				"end": 714,
				"text": "j"
			},
			"T18": {
				"eid": "T18",
				"label": "SYMBOL",
				"start": 761,
				"end": 762,
				"text": "k"
			},
			"T19": {
				"eid": "T19",
				"label": "PRIMARY",
				"start": 212,
				"end": 222,
				"text": "data point"
			},
			"T20": {
				"eid": "T20",
				"label": "PRIMARY",
				"start": 300,
				"end": 353,
				"text": "predicted probability that $x_i$ belongs to class $k$"
			},
			"T21": {
				"eid": "T21",
				"label": "PRIMARY",
				"start": 344,
				"end": 349,
				"text": "class"
			},
			"T22": {
				"eid": "T22",
				"label": "PRIMARY",
				"start": 369,
				"end": 387,
				"text": "a confusion matrix"
			},
			"T23": {
				"eid": "T23",
				"label": "PRIMARY",
				"start": 401,
				"end": 405,
				"text": "cell"
			},
			"T24": {
				"eid": "T24",
				"label": "PRIMARY",
				"start": 477,
				"end": 487,
				"text": "true class"
			},
			"T25": {
				"eid": "T25",
				"label": "PRIMARY",
				"start": 581,
				"end": 588,
				"text": "classes"
			},
			"T26": {
				"eid": "T26",
				"label": "PRIMARY",
				"start": 609,
				"end": 615,
				"text": "weight"
			},
			"T27": {
				"eid": "T27",
				"label": "PRIMARY",
				"start": 706,
				"end": 711,
				"text": "class"
			}
		},
		"relation": {
			"R4": {
				"label": "Direct",
				"arg0": "T19",
				"arg1": "T1",
				"rid": "R4"
			},
			"R5": {
				"label": "Direct",
				"arg0": "T20",
				"arg1": "T2",
				"rid": "R5"
			},
			"R11": {
				"label": "Direct",
				"arg0": "T21",
				"arg1": "T5",
				"rid": "R11"
			},
			"R12": {
				"label": "Direct",
				"arg0": "T22",
				"arg1": "T6",
				"rid": "R12"
			},
			"R13": {
				"label": "Direct",
				"arg0": "T23",
				"arg1": "T7",
				"rid": "R13"
			},
			"R14": {
				"label": "Direct",
				"arg0": "T24",
				"arg1": "T9",
				"rid": "R14"
			},
			"R16": {
				"label": "Direct",
				"arg0": "T24",
				"arg1": "T10",
				"rid": "R16"
			},
			"R19": {
				"label": "Direct",
				"arg0": "T25",
				"arg1": "T11",
				"rid": "R19"
			},
			"R22": {
				"label": "Direct",
				"arg0": "T25",
				"arg1": "T12",
				"rid": "R22"
			},
			"R23": {
				"label": "Direct",
				"arg0": "T26",
				"arg1": "T13",
				"rid": "R23"
			},
			"R24": {
				"label": "Direct",
				"arg0": "T26",
				"arg1": "T14",
				"rid": "R24"
			},
			"R27": {
				"label": "Direct",
				"arg0": "T27",
				"arg1": "T17",
				"rid": "R27"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T3",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T3",
				"arg1": "T4",
				"rid": "R501"
			},
			"R502": {
				"label": "Corefer-Symbol",
				"arg0": "T4",
				"arg1": "T8",
				"rid": "R502"
			},
			"R503": {
				"label": "Corefer-Symbol",
				"arg0": "T5",
				"arg1": "T12",
				"rid": "R503"
			},
			"R504": {
				"label": "Corefer-Symbol",
				"arg0": "T12",
				"arg1": "T18",
				"rid": "R504"
			},
			"R505": {
				"label": "Corefer-Symbol",
				"arg0": "T18",
				"arg1": "T15",
				"rid": "R505"
			},
			"R506": {
				"label": "Corefer-Symbol",
				"arg0": "T10",
				"arg1": "T11",
				"rid": "R506"
			},
			"R507": {
				"label": "Corefer-Symbol",
				"arg0": "T11",
				"arg1": "T16",
				"rid": "R507"
			},
			"R508": {
				"label": "Corefer-Symbol",
				"arg0": "T16",
				"arg1": "T17",
				"rid": "R508"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_39": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_39",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_39",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_39",
		"text": "These models yield $\\hat{R^2}$ measures of 0.7299 ( for logit ( AUC ) ) and 0.8009 ( for log ( LIFT ) ) , a strong indication that the variables we are considering are adequate to explain micro-experiment outcomes . The mixed effects models also provide coefficients measuring the effect of each problem dimension on each algorithm . Similar to \\cref{fig:ar,fig:pd,fig:cl,fig:ir} , \\cref{fig:arco,fig:pdco,fig:clco,fig:irco} show these coefficients across each problem dimension .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 20,
				"end": 29,
				"text": "\\hat{R^2}"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 31,
				"end": 39,
				"text": "measures"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_11": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_11",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_11",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_11",
		"text": "The well - known Local Outlier Factor algorithm ( Breunig , et al. \\cite{Breunig:2000} ) computes the outlier score of a point $x_i$ by computing its average distance to its $k$ nearest neighbors . It normalizes this distance by computing the average distance of each of those neighbors to {\\ it their } $k$ nearest neighbors . So , roughly speaking , a point is believed to be more anomalous if it is significantly farther from its neighbors than they are from each other .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 128,
				"end": 131,
				"text": "x_i"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 175,
				"end": 176,
				"text": "k"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 305,
				"end": 306,
				"text": "k"
			},
			"T4": {
				"eid": "T4",
				"label": "PRIMARY",
				"start": 119,
				"end": 126,
				"text": "a point"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 178,
				"end": 195,
				"text": "nearest neighbors"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T4",
				"arg1": "T1",
				"rid": "R1"
			},
			"R3": {
				"label": "Count",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R3"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T3",
				"rid": "R500"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_10": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_10",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_10",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_10",
		"text": "As proposed by Tax and Duin \\cite{Tax:2004} and similar in concept to One - Class SVM , Support Vector Data Description finds the smallest hypersphere in kernel space that encloses $1-\\delta$ of the data . As above , the outlier scores produced by this algorithm are determined by the residual after each point is projected onto the decision surface .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 182,
				"end": 190,
				"text": "1-\\delta"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_38": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_38",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_38",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_38",
		"text": "So far the $\\hat{R^2}$ of our best models are 0.5019 and 0.6382 which are respectable measures but leave a lot of variance unexplained . We constructed several mixed effect models to see if we could better explain our results with the same data . Our best models kept our problem dimensions as fixed effects and treated choice of algorithm and motherset as random effect groups . Each member of each random effect group models its interaction with the fixed effects . Additionally , choice of motherset also models its own interaction with choice of algorithm . $$metric \\sim (rf + pd + cl + ir | algo) + (rf + pd + cl + ir + algo | mset)$$",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 12,
				"end": 21,
				"text": "\\hat{R^2}"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 564,
				"end": 638,
				"text": "metric \\sim (rf + pd + cl + ir | algo) + (rf + pd + cl + ir + algo | mset)"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_12": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_12",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_12",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_12",
		"text": "Angle - Based Outlier Detection as proposed by Kriegel , et al in \\cite{abod} in its full form is an algorithm of cubic complexity as follows . For each point $x_i$ , consider all pairs of other points $(x_j,x_k)\\in X,i\\ne j\\ne k$ and compute the angle between them relative to to point $x_i$ . The sample variance of these angles determines the outlier score , with \\emph{lower} variances indicating anomalous points . Because of the run - time complexity , two simple approximations were suggested by the authors . The first is to sub sample the data and use this as the reference set for computing angles . The other is to only consider the angles among the $k$ nearest neighbors to $x_i$ . In initial experiments we found the latter to outperform the former and so that is the strategy employed in this study .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 160,
				"end": 163,
				"text": "x_i"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 203,
				"end": 212,
				"text": "(x_j,x_k)"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 204,
				"end": 207,
				"text": "x_j"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 208,
				"end": 211,
				"text": "x_k"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 216,
				"end": 219,
				"text": "X,i"
			},
			"T6": {
				"eid": "T6",
				"label": "SYMBOL",
				"start": 228,
				"end": 229,
				"text": "k"
			},
			"T7": {
				"eid": "T7",
				"label": "SYMBOL",
				"start": 288,
				"end": 291,
				"text": "x_i"
			},
			"T8": {
				"eid": "T8",
				"label": "SYMBOL",
				"start": 662,
				"end": 663,
				"text": "k"
			},
			"T9": {
				"eid": "T9",
				"label": "SYMBOL",
				"start": 687,
				"end": 690,
				"text": "x_i"
			},
			"T10": {
				"eid": "T10",
				"label": "PRIMARY",
				"start": 153,
				"end": 158,
				"text": "point"
			},
			"T11": {
				"eid": "T11",
				"label": "PRIMARY",
				"start": 195,
				"end": 201,
				"text": "points"
			},
			"T12": {
				"eid": "T12",
				"label": "PRIMARY",
				"start": 281,
				"end": 286,
				"text": "point"
			},
			"T13": {
				"eid": "T13",
				"label": "PRIMARY",
				"start": 665,
				"end": 682,
				"text": "nearest neighbors"
			}
		},
		"relation": {
			"R3": {
				"label": "Direct",
				"arg0": "T10",
				"arg1": "T1",
				"rid": "R3"
			},
			"R4": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T2",
				"rid": "R4"
			},
			"R5": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T3",
				"rid": "R5"
			},
			"R6": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T4",
				"rid": "R6"
			},
			"R7": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T5",
				"rid": "R7"
			},
			"R10": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T7",
				"rid": "R10"
			},
			"R11": {
				"label": "Direct",
				"arg0": "T12",
				"arg1": "T7",
				"rid": "R11"
			},
			"R12": {
				"label": "Count",
				"arg0": "T13",
				"arg1": "T8",
				"rid": "R12"
			},
			"R13": {
				"label": "Direct",
				"arg0": "T12",
				"arg1": "T9",
				"rid": "R13"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T7",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T7",
				"arg1": "T9",
				"rid": "R501"
			},
			"R502": {
				"label": "Corefer-Symbol",
				"arg0": "T6",
				"arg1": "T8",
				"rid": "R502"
			},
			"R503": {
				"label": "Corefer-Description",
				"arg0": "T10",
				"arg1": "T11",
				"rid": "R503"
			},
			"R504": {
				"label": "Corefer-Description",
				"arg0": "T11",
				"arg1": "T12",
				"rid": "R504"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_8": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_8",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_8",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_8",
		"text": "Another approach to density estimation is to fit a Gaussian mixture model ( GMM ) using the EM algorithm . A single GMM is not very robust , and like $k$ - means clustering it requires that we select a value of $k$ Gaussian mixture components . To improve robustness , we computed an ensemble of GMMs for many values of $k$ , discarded models that did not fit the data well , and then combined the predicted densities of the remaining models . As above , the outlier scores produced are based on the negative log - likelihood of each point according to the final models .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 76,
				"end": 79,
				"text": "GMM"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 116,
				"end": 119,
				"text": "GMM"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 151,
				"end": 152,
				"text": "k"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 212,
				"end": 213,
				"text": "k"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 321,
				"end": 322,
				"text": "k"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 51,
				"end": 73,
				"text": "Gaussian mixture model"
			},
			"T7": {
				"eid": "T7",
				"label": "PRIMARY",
				"start": 215,
				"end": 242,
				"text": "Gaussian mixture components"
			}
		},
		"relation": {
			"R2": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T1",
				"rid": "R2"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T2",
				"rid": "R3"
			},
			"R7": {
				"label": "Count",
				"arg0": "T7",
				"arg1": "T4",
				"rid": "R7"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T2",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T3",
				"arg1": "T4",
				"rid": "R501"
			},
			"R502": {
				"label": "Corefer-Symbol",
				"arg0": "T4",
				"arg1": "T5",
				"rid": "R502"
			},
			"R503": {
				"label": "Corefer-Description",
				"arg0": "T6",
				"arg1": "T7",
				"rid": "R503"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_9": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_9",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_9",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_9",
		"text": "The One - Class SVM algorithm ( Scholkopf et al. \\cite{Scholkopf:99} ) uses a Support - Vector Machine to search for a kernel - space decision boundary that separates fraction $1-\\delta$ of the data from the kernel - space origin . The outlier scores produced by this algorithm are determined by the residual after each point is projected onto the decision surface . Points outside the decision boundary will have positive residuals , where interior points will have negative residuals .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 177,
				"end": 185,
				"text": "1-\\delta"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 167,
				"end": 175,
				"text": "fraction"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T2",
				"arg1": "T1",
				"rid": "R1"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_13": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_13",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_13",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_13",
		"text": "Precision - at - $k$ or Recall - at - $k$ metrics are also common but run the risk of being application specific ( the appropriate selection of $k$ is determined by the context of the application ) and we feel not appropriate for a broad meta-analysis such as this , but we note that such metrics might be the more useful in a real - world setting .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 18,
				"end": 19,
				"text": "k"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 24,
				"end": 40,
				"text": "Recall - at - $k"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 39,
				"end": 40,
				"text": "k"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 145,
				"end": 146,
				"text": "k"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 42,
				"end": 49,
				"text": "metrics"
			}
		},
		"relation": {
			"R3": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T1",
				"rid": "R3"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T3",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T3",
				"arg1": "T4",
				"rid": "R501"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_17": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_17",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_17",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_17",
		"text": "\\cref{tbl:hypo} summarizes the global benchmark failure rates for AUC and AP . The `` Either ' ' column indicates benchmarks for which all algorithms failed under at least one of the two metrics . \\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Significance Level\\label{tbl:hypo} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline $\\alpha=0.05$ & 0.2418 & 0.2815 & 0.3337 \\\\ $\\alpha=0.01$ & 0.3282 & 0.4162 & 0.4609 \\\\ $\\alpha=0.001$ & 0.4108 & 0.5087 & 0.5430 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 471,
				"end": 477,
				"text": "\\alpha"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 515,
				"end": 521,
				"text": "\\alpha"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 559,
				"end": 565,
				"text": "\\alpha"
			}
		},
		"relation": {
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T2",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T3",
				"rid": "R501"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_16": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_16",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_16",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_16",
		"text": "We performed such hypothesis tests for both AUC and AP and for $\\alpha\\in(0.05,0.01,0.001)$ for each micro-experiment result . Failure in this context refers to failing to reject the null hypothesis . To summarize our tests , we first define the notion of \\textbf{benchmark failure} as a benchmark instance for which \\emph{all} algorithms failed . While benchmark failure is dependent on the algorithms used in this study , we still believe it is a good indication that we should not use the benchmark as evidence for later conclusions .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 64,
				"end": 70,
				"text": "\\alpha"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_14": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_14",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_14",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_14",
		"text": "Specifically , one can treat the AUC and AP of a random ranking of points as random variables each with parameters $n_{\\text{anom}}$ ( the number of anomalies in the benchmark ) and $n_{\\text{norm}}$ ( the number of normals ) and then compute the quantiles of interest for each of these distributions . Conducting a test with significance $\\alpha$ is a matter of computing the $(1-\\alpha)$ - quantile of the appropriate random variable . Refer to \\cref{appendix:random} for an overview of how AUC and AP can be treated as random variables .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 116,
				"end": 131,
				"text": "n_{\\text{anom}}"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 183,
				"end": 198,
				"text": "n_{\\text{norm}}"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 340,
				"end": 346,
				"text": "\\alpha"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 378,
				"end": 388,
				"text": "(1-\\alpha)"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 104,
				"end": 114,
				"text": "parameters"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 326,
				"end": 338,
				"text": "significance"
			},
			"T7": {
				"eid": "T7",
				"label": "PRIMARY",
				"start": 392,
				"end": 400,
				"text": "quantile"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R2"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T3",
				"rid": "R3"
			},
			"R4": {
				"label": "Direct",
				"arg0": "T7",
				"arg1": "T4",
				"rid": "R4"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_28": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_28",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_28",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_28",
		"text": "To demonstrate the impact of our benchmark construction methodology on micro-experiment results we first want to examine results in a way that is agnostic to choice of algorithm . Because we are only examining results from benchmarks where at least one algorithm produced statistically significant output , we know that if we only consider the best result from each benchmark we are always choosing a result that is better than random with high confidence . As each benchmark construction factor has a well defined control group , we compute the mean difference in performance - of - best - algorithm between each level and the control group , and then place a $0.999$ confidence interval around this difference . The results are displayed in \\cref{fig:mset,fig:ar,fig:pd,fig:cl,fig:ir} . Observe that the metrics logit ( AUC ) and log ( LIFT ) are not meant to be compared to each other , but are shown side by side for a more compact presentation .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 662,
				"end": 667,
				"text": "0.999"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 669,
				"end": 688,
				"text": "confidence interval"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T2",
				"arg1": "T1",
				"rid": "R1"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_29": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_29",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_29",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_29",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric\\label{tbl:meanalgo} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean logit [unused10] } & \\textbf{Mean [unused10] } \\\\ \\hline \\hline abod & 0.9517 & 0.9009 \\\\ egmm & 0.9678 & 0.9081\\\\ iforest & \\textbf{1.0893} & [unused10] \\\\ loda & 0.8604 & 0.9156 \\\\ lof & 0.9632 & 0.9329\\\\ ocsvm & 0.5650 & 0.8608 \\\\ rkde & 0.9554 & 0.9132\\\\ svdd & 0.1538 & 0.2806 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_15": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_15",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_15",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_15",
		"text": "A more intuitive view of this process : if algorithm $a$ achieves an AUC 0 f 0.75 on benchmark $b$ and we reject the null hypothesis with $\\alpha=0.001$ , that would mean that with probability at least 0.999 a random ranking would achieve an AUC worse than 0.75 \\emph{on benchmark [unused10] } .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 54,
				"end": 55,
				"text": "a"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 96,
				"end": 97,
				"text": "b"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 139,
				"end": 145,
				"text": "\\alpha"
			},
			"T4": {
				"eid": "T4",
				"label": "PRIMARY",
				"start": 43,
				"end": 52,
				"text": "algorithm"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 85,
				"end": 94,
				"text": "benchmark"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T4",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R2"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_18": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_18",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_18",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_18",
		"text": "The appropriate significance level for this study is debatable . Smaller $\\alpha$ trades away potential evidence ( by eliminating benchmarks ) for greater confidence that the results from the benchmarks under consideration are relevant . We choose to apply the more stringent threshold of $\\alpha=0.001$ ; even though the failure rate at this level is rather high , it still leaves many benchmarks across all factors of interest .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 74,
				"end": 80,
				"text": "\\alpha"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 290,
				"end": 296,
				"text": "\\alpha"
			}
		},
		"relation": {
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T2",
				"rid": "R500"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_30": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_30",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_30",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_30",
		"text": "We have already described measures of \\textbf{clusteredness} and \\textbf{feature irrelevance} in \\cref{sec:method} that are continuous ; as with clusteredness we will apply a log transform to the feature irrelevance ratio . \\textbf{relative frequency} and \\textbf{point difficulty} are both in the range $[0,1]$ and so as with AUC we will apply the logit transform to them . This gives us real valued representations of each problem dimension suitable for a linear model .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 305,
				"end": 310,
				"text": "[0,1]"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 298,
				"end": 303,
				"text": "range"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T2",
				"arg1": "T1",
				"rid": "R1"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_24": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_24",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_24",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_24",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Feature Irrelevance Level ( [unused10] )\\label{tbl:irfail} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline ir - 0 & 0.3065 & 0.4008 & 0.4327 \\\\ ir - 1 & 0.3785 & 0.4760 & 0.5098 \\\\ ir - 2 & \\textbf{0.4356} & \\textbf{0.5362} & \\textbf{0.5743} \\\\ ir - 3 & \\textbf{0.5224} & \\textbf{0.6215} & \\textbf{0.6549} \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_2": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_2",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_2",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_2",
		"text": "{ \\it Relative frequency} is the fraction of the incoming data points that are anomalies of interest. This value is the problem dimension that is most reliably reported in the literature already and has also been called ``plurality'' and ``contamination rate''. Little is done to examine the impact is has on results, however. The behavior of anomaly detection algorithms often changes with the relative frequency. If anomalies are very rare, then methods that pretend that all training points are ``normal'' and fit a model to them may do well. If anomalies are more common, then methods that attempt to fit a model of the anomalies may do well. In most experiments in the literature, the anomalies have a relative frequency between 0.01 and 0.1, but some go as high as 0.3 \\cite{kim:08,Liu:08} . Many security applications are estimated to have relative frequencies in the range of $10^{-5}$ or $10^{-6}$ .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 885,
				"end": 892,
				"text": "10^{-5}"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 898,
				"end": 905,
				"text": "10^{-6}"
			},
			"T3": {
				"eid": "T3",
				"label": "PRIMARY",
				"start": 847,
				"end": 867,
				"text": "relative frequencies"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T3",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T3",
				"arg1": "T2",
				"rid": "R2"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_3": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_3",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_3",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_3",
		"text": "We propose to define point difficulty ( pd ) based on an oracle that knows the true generating processes underlying the `` normal ' ' and `` anomalous ' ' points . Using this knowledge , the oracle can estimate the probability $P(y=\\mbox{normal}|x)$ that a data point $x$ was generated by the `` normal ' ' distribution or $P(y=\\mbox{anomaly}|x)$ that a data point $x$ was generated by the `` anomalous ' ' distribution . We consider the point difficulty of any point to be the estimated probability that it belongs to the other class . The point difficulty level of an entire benchmark is summarized as the mean of the point difficulty of all the points in the benchmark .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 228,
				"end": 248,
				"text": "P(y=\\mbox{normal}|x)"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 269,
				"end": 270,
				"text": "x"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 324,
				"end": 345,
				"text": "P(y=\\mbox{anomaly}|x)"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 366,
				"end": 367,
				"text": "x"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 215,
				"end": 226,
				"text": "probability"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 255,
				"end": 267,
				"text": "a data point"
			},
			"T7": {
				"eid": "T7",
				"label": "PRIMARY",
				"start": 352,
				"end": 364,
				"text": "a data point"
			}
		},
		"relation": {
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T1",
				"rid": "R2"
			},
			"R4": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T2",
				"rid": "R4"
			},
			"R5": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T3",
				"rid": "R5"
			},
			"R6": {
				"label": "Direct",
				"arg0": "T7",
				"arg1": "T4",
				"rid": "R6"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T3",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T4",
				"rid": "R501"
			},
			"R502": {
				"label": "Corefer-Description",
				"arg0": "T6",
				"arg1": "T7",
				"rid": "R502"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_25": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_25",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_25",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_25",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Algorithm Failure Rate by Metric ( [unused10] )\\label{tbl:algofail} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline abod & 0.5898 & 0.6784 & 0.7000 \\\\ iforest & \\textbf{0.5520} & \\textbf{0.6514} & [unused10] \\\\ loda & 0.6187 & 0.6955 & 0.7194 \\\\ lof & 0.6016 & 0.7071 & 0.7331 \\\\ rkde & 0.6122 & 0.7030 & 0.7194 \\\\ ocsvm & 0.7218 & 0.7342 & 0.7960 \\\\ svdd & 0.8482 & 0.8868 & 0.9080 \\\\ egmm & 0.6188 & 0.7146 & 0.7303 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_31": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_31",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_31",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_31",
		"text": "The simplest fixed effect linear model to build would be to predict a metric given our four problem dimensions ( abbreviated $rf,pd,cl,ir$ ) , choice of motherset ( $mset$ ) and choice of algorithm ( $algo$ ) . $$metric \\sim rf + pd + cl + ir + mset + algo$$",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 126,
				"end": 137,
				"text": "rf,pd,cl,ir"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 166,
				"end": 170,
				"text": "mset"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 201,
				"end": 205,
				"text": "algo"
			},
			"T4": {
				"eid": "T4",
				"label": "PRIMARY",
				"start": 92,
				"end": 110,
				"text": "problem dimensions"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 143,
				"end": 162,
				"text": "choice of motherset"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 178,
				"end": 197,
				"text": "choice of algorithm"
			},
			"T7": {
				"eid": "T7",
				"label": "PRIMARY",
				"start": 213,
				"end": 219,
				"text": "metric"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T4",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R2"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T3",
				"rid": "R3"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_19": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_19",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_19",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_19",
		"text": "\\begin{table}  \\resizebox{\\textwidth} { ! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Motherset ( [unused10] )\\label{tbl:msetfail} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline synthetic & 0.2417 & 0.2553 & 0.2765 \\\\ abalone & 0.3838 & 0.4533 & 0.4841 \\\\ comm. and .crime & \\textbf{0.4924} & 0.5053 & [unused10] \\\\ concrete & \\textbf{0.5121} & \\textbf{0.5908} & [unused10] \\\\ fault & \\textbf{0.4178} & \\textbf{0.6105} & [unused10] \\\\ gas & 0.2767 & \\textbf{0.5178} & 0.5189 \\\\ imgseg & 0.2792 & 0.2569 & 0.3667 \\\\ landsat & \\textbf{0.5286} & \\textbf{0.5617} & [unused10] \\\\ letter .rec & \\textbf{0.6276} & \\textbf{0.7273} & [unused10] \\\\ magic .gamma & 0.3300 & 0.3322 & 0.3917 \\\\ opt .digits & \\textbf{0.5567} & \\textbf{0.7115} & [unused10] \\\\ pageb & 0.0468 & 0.1894 & 0.1915\\\\ particle & 0.2533 & 0.3678 & 0.4306 \\\\ shuttle & 0.0944 & 0.2711 & 0.2722 \\\\ skin & 0.0853 & 0.3673 & 0.3693 \\\\ spambase & 0.3622 & 0.4844 & 0.5178 \\\\ wave & \\textbf{0.4954} & \\textbf{0.6000} & \\textbf{0.6278} \\\\ wine & \\textbf{0.4860} & \\textbf{0.6355} & \\textbf{0.6554} \\\\ yearp & \\textbf{0.6822} & \\textbf{0.7239} & \\textbf{0.7572} \\\\ yeast & \\textbf{0.9733} & \\textbf{0.9789} & \\textbf{0.9900} \\\\ \\ hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 926,
				"end": 933,
				"text": "shuttle"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_27": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_27",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_27",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_27",
		"text": "\\begin{itemize}  \\item \\textbf{AUC} -- We use the logit transform : $$\\text{logit}(AUC)=\\log\\left(\\frac{AUC}{1-AUC}\\right)$$ \\item \\textbf{AP} -- Because AP does not have a constant expectation , one way to normalize AP is to compute the \\textbf{lift} which is the ratio of AP to its expectation . It is commonly assumed that this expectation is equivalent to the anomaly rate , but \\cite{exactAP} shows that while this can be a good approximation it is not exactly correct . More important it can be a bad approximation when relative frequency is low , which is the case for most of our benchmarks . We compute lift using the exact expectation . To map this ratio to all real numbers a sensible further transformation is to take the log of this lift : $$\\log(LIFT)=\\log\\left(\\frac{AP}{E[AP]}\\right)$$ \\end{itemize}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 70,
				"end": 87,
				"text": "\\text{logit}(AUC)"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 88,
				"end": 122,
				"text": "\\log\\left(\\frac{AUC}{1-AUC}\\right)"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 755,
				"end": 765,
				"text": "\\log(LIFT)"
			},
			"T4": {
				"eid": "T4",
				"label": "PRIMARY",
				"start": 46,
				"end": 65,
				"text": "the logit transform"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T4",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T4",
				"arg1": "T2",
				"rid": "R2"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_33": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_33",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_33",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_33",
		"text": "An ANOVA test on each of these models provides a $t$ - test for each variable in the model and an $F$ - test on the model itself . To save space we do not report these individual values ; the results are easily summarized as \\emph{every} test has a $p$ - value well below $0.001$ .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 50,
				"end": 51,
				"text": "t"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 99,
				"end": 100,
				"text": "F"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 250,
				"end": 251,
				"text": "p"
			},
			"T4": {
				"eid": "T4",
				"label": "PRIMARY",
				"start": 55,
				"end": 59,
				"text": "test"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 104,
				"end": 108,
				"text": "test"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 255,
				"end": 260,
				"text": "value"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T4",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R2"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_1": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_1",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_1",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_1",
		"text": "An anomaly detection algorithm takes as input the $N$ data points and produces as output a real - valued anomaly score for each point such that points with higher scores are believed to be more anomalous . Natural metrics of quality for anomaly predictions are the area under the ROC curve ( AUC ) and the Average Precision ( AP ; also known as the area under the precision - recall curve ) . In some applications , we are only interested in the top $K$ highest - ranked points , in which case , natural metrics are the precision and recall at $K$ . In other applications , we might choose a threshold and classify all points whose anomaly score exceeds the threshold as anomalies and all other points as nominal . In such settings , common metrics are precision , recall , and F1 ( the harmonic mean of precision and recall ) . Accuracy or error rate are typically not very useful , because in most applications the anomalies constitute a very small fraction of the data ( e.g. , from 0.01\\% to 1\\% ) . In this paper , we consider only the AUC and AP metrics .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 51,
				"end": 52,
				"text": "N"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 292,
				"end": 295,
				"text": "AUC"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 451,
				"end": 452,
				"text": "K"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 545,
				"end": 546,
				"text": "K"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 1041,
				"end": 1044,
				"text": "AUC"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 54,
				"end": 65,
				"text": "data points"
			},
			"T7": {
				"eid": "T7",
				"label": "PRIMARY",
				"start": 265,
				"end": 289,
				"text": "area under the ROC curve"
			},
			"T8": {
				"eid": "T8",
				"label": "PRIMARY",
				"start": 302,
				"end": 323,
				"text": "the Average Precision"
			},
			"T9": {
				"eid": "T9",
				"label": "PRIMARY",
				"start": 454,
				"end": 477,
				"text": "highest - ranked points"
			}
		},
		"relation": {
			"R1": {
				"label": "Count",
				"arg0": "T6",
				"arg1": "T1",
				"rid": "R1"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T7",
				"arg1": "T2",
				"rid": "R3"
			},
			"R5": {
				"label": "Count",
				"arg0": "T9",
				"arg1": "T3",
				"rid": "R5"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T5",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T3",
				"arg1": "T4",
				"rid": "R501"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_0": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_0",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_0",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_0",
		"text": "We study the following unsupervised anomaly detection setting . We are given a collection of $N$ data points $x_1, \\ldots, x_N$ , each a $d$ - dimensional real - valued vector . These data points are a mixture of `` nominal ' ' points and `` anomalous ' ' points . However , none of the points are labeled . The goal is to identify the anomalous points .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 94,
				"end": 95,
				"text": "N"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 123,
				"end": 126,
				"text": "x_N"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 135,
				"end": 136,
				"text": "a"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 138,
				"end": 139,
				"text": "d"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 97,
				"end": 108,
				"text": "data points"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 143,
				"end": 175,
				"text": "dimensional real - valued vector"
			}
		},
		"relation": {
			"R1": {
				"label": "Count",
				"arg0": "T5",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R2"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T3",
				"rid": "R3"
			},
			"R4": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T4",
				"rid": "R4"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_32": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_32",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_32",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_32",
		"text": "First we build this model for each metric , using our discrete construction factors and our real valued transformations and compare the models . Our metric for comparison is the $\\hat{R^2}$ goodness - of - fit measure , which is inversely related to the mean squared error of the model , which is the figure of merit each of these models is trying to optimize .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 179,
				"end": 188,
				"text": "\\hat{R^2}"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 190,
				"end": 217,
				"text": "goodness - of - fit measure"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T2",
				"arg1": "T1",
				"rid": "R1"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_26": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_26",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_26",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_26",
		"text": "Later in this section we will to analyze our micro-experiment results with various linear models , and summarizing the means of metrics like AUC and AP which are constrained to range $[0,1]$ can be problematic , especially in the case of AP which does not have a constant expectation . For the remainder of this paper we transform both of these metrics so that they extend to the range of all real numbers in the following ways :",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 184,
				"end": 189,
				"text": "[0,1]"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 177,
				"end": 182,
				"text": "range"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T2",
				"arg1": "T1",
				"rid": "R1"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_22": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_22",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_22",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_22",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\ text width } \\tbl{Benchmark Failure Rate by Metric and Point Difficulty Level ( [unused10] )\\label{tbl:pdfail} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline pd - 0 & 0.2887 & 0.3951 & 0.4328 \\\\ pd - 1 & 0.2803 & 0.3988 & 0.4268 \\\\ pd - 2 & \\textbf{0.4252} & \\textbf{0.5257} & \\textbf{0.5576} \\\\ pd - 3 & \\textbf{0.5662} & \\textbf{0.6481} & \\textbf{0.6858} \\\\ pd - 4 & \\textbf{0.7540} & \\textbf{0.8041} & \\textbf{0.8437} \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 295,
				"end": 301,
				"text": "pd - 0"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 431,
				"end": 439,
				"text": "\\ pd - 3"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 495,
				"end": 503,
				"text": "\\ pd - 4"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_36": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_36",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_36",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_36",
		"text": "To better evaluate the importance of each variable on predicting our metrics we construct simpler models that exclude one of the variables from the model and then measure the difference in $\\hat{R^2}$ measures relative to our base model . We also construct a model without all four problem dimension variables to measure the impact of all problem dimensions in aggregate . \\cref{tbl:rsq} shows these results . Boldfaced items are those with greater $\\hat{R^2}$ loss than the algorithm variable which suggests that the variable is \\emph{more important to your final outcome than your choice of algorithm is} .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 190,
				"end": 199,
				"text": "\\hat{R^2}"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 450,
				"end": 459,
				"text": "\\hat{R^2}"
			},
			"T3": {
				"eid": "T3",
				"label": "PRIMARY",
				"start": 201,
				"end": 209,
				"text": "measures"
			}
		},
		"relation": {
			"R2": {
				"label": "Direct",
				"arg0": "T3",
				"arg1": "T1",
				"rid": "R2"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T2",
				"rid": "R500"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_4": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_4",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_4",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_4",
		"text": "We binned this measure into five discrete levels : \\begin{itemize}  \\item \\emph{pd-0} : control group ; ( difficulty score $\\in(0,1))$ \\item \\emph{pd-1} : difficulty score $\\in(0,0.1\\overline{6})$ \\item \\emph{pd-2} : difficulty score $\\in[0.1\\overline{6},0.\\overline{3})$ \\item \\emph{pd-3} : difficulty score $\\in[0.\\overline{3},0.5)$ \\item \\emph{pd-4} : difficulty score $\\in[0.5,1)$ \\end{itemize}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 124,
				"end": 132,
				"text": "\\in(0,1)"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 141,
				"end": 152,
				"text": "\\emph{pd-1}"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 173,
				"end": 195,
				"text": "\\in(0,0.1\\overline{6})"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 203,
				"end": 214,
				"text": "\\emph{pd-2}"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 235,
				"end": 270,
				"text": "\\in[0.1\\overline{6},0.\\overline{3})"
			},
			"T6": {
				"eid": "T6",
				"label": "SYMBOL",
				"start": 278,
				"end": 289,
				"text": "\\emph{pd-3}"
			},
			"T7": {
				"eid": "T7",
				"label": "SYMBOL",
				"start": 310,
				"end": 333,
				"text": "\\in[0.\\overline{3},0.5)"
			},
			"T8": {
				"eid": "T8",
				"label": "SYMBOL",
				"start": 310,
				"end": 352,
				"text": "\\in[0.\\overline{3},0.5)$ \\item \\emph{pd-4}"
			},
			"T9": {
				"eid": "T9",
				"label": "SYMBOL",
				"start": 341,
				"end": 352,
				"text": "\\emph{pd-4}"
			},
			"T10": {
				"eid": "T10",
				"label": "SYMBOL",
				"start": 373,
				"end": 383,
				"text": "\\in[0.5,1)"
			},
			"T11": {
				"eid": "T11",
				"label": "PRIMARY",
				"start": 106,
				"end": 122,
				"text": "difficulty score"
			},
			"T12": {
				"eid": "T12",
				"label": "PRIMARY",
				"start": 155,
				"end": 171,
				"text": "difficulty score"
			},
			"T13": {
				"eid": "T13",
				"label": "PRIMARY",
				"start": 217,
				"end": 233,
				"text": "difficulty score"
			},
			"T14": {
				"eid": "T14",
				"label": "PRIMARY",
				"start": 292,
				"end": 308,
				"text": "difficulty score"
			},
			"T15": {
				"eid": "T15",
				"label": "PRIMARY",
				"start": 355,
				"end": 371,
				"text": "difficulty score"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T3",
				"rid": "R2"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T12",
				"arg1": "T3",
				"rid": "R3"
			},
			"R4": {
				"label": "Direct",
				"arg0": "T13",
				"arg1": "T5",
				"rid": "R4"
			},
			"R5": {
				"label": "Direct",
				"arg0": "T14",
				"arg1": "T7",
				"rid": "R5"
			},
			"R6": {
				"label": "Direct",
				"arg0": "T14",
				"arg1": "T8",
				"rid": "R6"
			},
			"R7": {
				"label": "Direct",
				"arg0": "T15",
				"arg1": "T10",
				"rid": "R7"
			},
			"R500": {
				"label": "Corefer-Description",
				"arg0": "T11",
				"arg1": "T12",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Description",
				"arg0": "T12",
				"arg1": "T13",
				"rid": "R501"
			},
			"R502": {
				"label": "Corefer-Description",
				"arg0": "T13",
				"arg1": "T14",
				"rid": "R502"
			},
			"R503": {
				"label": "Corefer-Description",
				"arg0": "T14",
				"arg1": "T15",
				"rid": "R503"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_5": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_5",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_5",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_5",
		"text": "We propose to define semantic variation among anomalies with the following measure . The { \\ it normalized clusteredness } ( nc ) of this set of points is defined as \\[\\log\\left(\\frac{\\hat{\\sigma} ^ 2_n } { \\hat{\\sigma} ^ 2_a }\\ right ) \\ ] where $\\hat{\\sigma}^2_n$ is the sample variance of the selected normal points and $\\hat{\\sigma}^2_a$ is the sample variance of the selected anomaly points . When normalized clusteredness is less than 0 , the anomaly points exhibit greater semantic variation than the normal points . When normalized clusteredness is greater than 0 , the anomaly points are more tightly packed than the normal points ( on average ) .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 184,
				"end": 202,
				"text": "\\hat{\\sigma} ^ 2_n"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 207,
				"end": 225,
				"text": "\\hat{\\sigma} ^ 2_a"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 248,
				"end": 264,
				"text": "\\hat{\\sigma}^2_n"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 324,
				"end": 340,
				"text": "\\hat{\\sigma}^2_a"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 85,
				"end": 151,
				"text": "The { \\ it normalized clusteredness } ( nc ) of this set of points"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 96,
				"end": 120,
				"text": "normalized clusteredness"
			},
			"T7": {
				"eid": "T7",
				"label": "PRIMARY",
				"start": 273,
				"end": 318,
				"text": "sample variance of the selected normal points"
			},
			"T8": {
				"eid": "T8",
				"label": "PRIMARY",
				"start": 349,
				"end": 395,
				"text": "sample variance of the selected anomaly points"
			}
		},
		"relation": {
			"R3": {
				"label": "Direct",
				"arg0": "T7",
				"arg1": "T3",
				"rid": "R3"
			},
			"R4": {
				"label": "Direct",
				"arg0": "T8",
				"arg1": "T4",
				"rid": "R4"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T3",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T4",
				"rid": "R501"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_37": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_37",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_37",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_37",
		"text": "\\begin{table} \\resizebox{\\textwidth} { ! } { \\begin{minipage} { 1.15 \\ text width } \\tbl{Changes in [unused10] When Different Variables Are Missing\\label{tbl:rsq} } { \\begin{tabular} { | r | | c | c | c | c | } \\hline & \\textbf{ [unused10] } & \\textbf{ [unused10] } & \\textbf{ [unused10] } & \\textbf{ [unused10] } \\\\ \\hline \\hline All Variables & 0.5019 & -- & 0.6382 & --\\\\ w/o Algorithm & 0.4512 & 0.0507 & 0.6013 & 0.0369 \\\\ w/o Motherset & \\textbf{0.2617} & \\textbf{0.2403} & \\textbf{0.5190} & [unused10] \\\\ w/o All Problem Dimensions & \\textbf{0.3221} & \\textbf{0.1799} & \\textbf{0.2359} & \\textbf{0.4022} \\\\ -- w/o Relative Frequency & \\textbf{0.4311} & \\textbf{0.0709} & \\textbf{0.4108} & \\textbf{0.2274} \\\\ -- w/o Point Difficulty & 0.4742 & 0.0277 & 0.6264 & 0.0117 \\\\ -- w/o Clusteredness & 0.4909 & 0.0111 & \\textbf{0.6004} & \\textbf{0.0378} \\\\ -- w /o Feature Irrelevance & 0.4831 & 0.0189 & 0.6279 & 0.0103 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "PRIMARY",
				"start": 331,
				"end": 344,
				"text": "All Variables"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 331,
				"end": 388,
				"text": "All Variables & 0.5019 & -- & 0.6382 & --\\\\ w/o Algorithm"
			},
			"T3": {
				"eid": "T3",
				"label": "PRIMARY",
				"start": 617,
				"end": 639,
				"text": "w/o Relative Frequency"
			},
			"T4": {
				"eid": "T4",
				"label": "PRIMARY",
				"start": 718,
				"end": 738,
				"text": "w/o Point Difficulty"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 781,
				"end": 798,
				"text": "w/o Clusteredness"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_23": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_23",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_23",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_23",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Clustering Strategy ( [unused10] )\\label{tbl:clfail} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline none & \\textbf{0.4189} & \\textbf{0.5201} & \\textbf{0.5479} \\\\ cluster & \\textbf{0.4431} & \\textbf{0.5891} & \\textbf{0.6036} \\\\ scatter & 0.3695 & 0.4146 & 0.4759 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_35": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_35",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_35",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_35",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.55\\ text width } \\tbl{Problem Dimension Coefficients by Metric\\label{tbl:basecoef} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{ [unused10] } & \\textbf{ [unused10] } \\\\ \\hline \\hline $\\text{logit}(\\text{relative frequency})$ & - 0.1994 & - 0.3527 \\\\ $\\text{logit}(\\text{point difficulty})$ & - 0.3209 & - 0.2014 \\\\ $\\text{clusteredness}$ & - 0.1255 & - 0.2141 \\\\ $\\log(\\text{feature irrelevance})$ & - 0.2962 & - 0.1998 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 262,
				"end": 301,
				"text": "\\text{logit}(\\text{relative frequency})"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 329,
				"end": 366,
				"text": "\\text{logit}(\\text{point difficulty})"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 442,
				"end": 474,
				"text": "\\log(\\text{feature irrelevance})"
			}
		},
		"relation": {
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T2",
				"rid": "R500"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_21": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_21",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_21",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_21",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Relative Frequency Level ( [unused10] )\\label{tbl:arfail} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline rf - 0 & 0.3432 & 0.4212 & 0.4324 \\\\ rf - 1 & \\textbf{0.7286} & \\textbf{0.8464} & \\textbf{0.9081} \\\\ rf - 2 & \\textbf{0.5432} & \\textbf{0.6889} & [unused10] \\\\ rf - 3 & \\textbf{0.4395} & \\textbf{0.5786} & \\textbf{0.6395} \\\\ rf - 4 & 0.2350 & 0.3076 & 0.3234 \\\\ rf - 5 & 0.2533 & 0.3031 & 0.3139 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 296,
				"end": 302,
				"text": "rf - 0"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_7": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_7",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_7",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_7",
		"text": "To simplify the process of determining how many irrelevant features are needed , we compute an estimate of how many extra features will achieve a desired average distance ratio . Note that the expected distance between two vectors ( $\\alpha$ ) whose coordinates are drawn at random ( e.g. , from the unit interval or from a standard normal Gaussian ) grows in proportion to $\\sqrt{d}$ , where $d$ is the dimensionality of the data . Hence , if a dataset already has $d$ dimensions and we want to estimate $d'$ , the number of dimensions needed to increase the average pairwise distance by a factor of $\\alpha$ , then we need $$\\hat{d'} = (\\alpha \\sqrt{d})^2$$ dimensions , where $\\alpha\\in\\{1.0,1.2,1.5,2.0\\}$ for this study .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 234,
				"end": 240,
				"text": "\\alpha"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 381,
				"end": 382,
				"text": "d"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 394,
				"end": 395,
				"text": "d"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 467,
				"end": 468,
				"text": "d"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 506,
				"end": 508,
				"text": "d'"
			},
			"T6": {
				"eid": "T6",
				"label": "SYMBOL",
				"start": 602,
				"end": 608,
				"text": "\\alpha"
			},
			"T7": {
				"eid": "T7",
				"label": "SYMBOL",
				"start": 632,
				"end": 634,
				"text": "d'"
			},
			"T8": {
				"eid": "T8",
				"label": "SYMBOL",
				"start": 639,
				"end": 645,
				"text": "\\alpha"
			},
			"T9": {
				"eid": "T9",
				"label": "SYMBOL",
				"start": 652,
				"end": 653,
				"text": "d"
			},
			"T10": {
				"eid": "T10",
				"label": "SYMBOL",
				"start": 680,
				"end": 686,
				"text": "\\alpha"
			},
			"T11": {
				"eid": "T11",
				"label": "PRIMARY",
				"start": 189,
				"end": 230,
				"text": "the expected distance between two vectors"
			},
			"T12": {
				"eid": "T12",
				"label": "PRIMARY",
				"start": 400,
				"end": 430,
				"text": "the dimensionality of the data"
			},
			"T13": {
				"eid": "T13",
				"label": "PRIMARY",
				"start": 470,
				"end": 480,
				"text": "dimensions"
			},
			"T14": {
				"eid": "T14",
				"label": "PRIMARY",
				"start": 591,
				"end": 597,
				"text": "factor"
			}
		},
		"relation": {
			"R4": {
				"label": "Direct",
				"arg0": "T11",
				"arg1": "T1",
				"rid": "R4"
			},
			"R10": {
				"label": "Direct",
				"arg0": "T12",
				"arg1": "T3",
				"rid": "R10"
			},
			"R12": {
				"label": "Count",
				"arg0": "T13",
				"arg1": "T4",
				"rid": "R12"
			},
			"R16": {
				"label": "Direct",
				"arg0": "T14",
				"arg1": "T6",
				"rid": "R16"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T6",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T6",
				"arg1": "T8",
				"rid": "R501"
			},
			"R502": {
				"label": "Corefer-Symbol",
				"arg0": "T8",
				"arg1": "T10",
				"rid": "R502"
			},
			"R503": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T3",
				"rid": "R503"
			},
			"R504": {
				"label": "Corefer-Symbol",
				"arg0": "T3",
				"arg1": "T4",
				"rid": "R504"
			},
			"R505": {
				"label": "Corefer-Symbol",
				"arg0": "T4",
				"arg1": "T9",
				"rid": "R505"
			},
			"R506": {
				"label": "Corefer-Symbol",
				"arg0": "T5",
				"arg1": "T7",
				"rid": "R506"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_6": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_6",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_6",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_6",
		"text": "\\begin{itemize} \\item \\emph{nc-0} : control group ; ( clusteredness not considered ) . \\item \\emph{nc-1} : nc $< 0$ ; ( scattered anomalies ) . \\item \\emph{nc-2} : nc $> 0$ ; ( clustered anomalies ) . \\end{itemize}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_20": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_20",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_20",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_20",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\ text width } \\tbl{Benchmark Failure Rate by Metric and Motherset Origin ( [unused10] )\\label{tbl:origfail} } { \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline binary & 0.2490 & 0.3530 & 0.3914 \\\\ multiclass & \\textbf{0.4488} & \\textbf{0.5627} & \\textbf{0.5913} \\\\ regression & \\textbf{0.5159} & \\textbf{0.5830} & \\textbf{0.6219} \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_34": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_34",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_34",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_34",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\ text width } \\tbl{ [unused10] of Linear Regression by Metric and Variable Type\\label{tbl:facvval} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{ [unused10] } & \\textbf{ [unused10] } \\\\ \\hline \\hline Discrete Variables & 0.4910 & 0.6251 \\\\ Real Variables & 0.5019 & 0.6382 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_53": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_53",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_53",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_53",
		"text": "As described in the main body of the paper we used a KNN approximation of the original algorithm . The only parameter in such an implementation is the choice of $k$ , which we set at 0.005 of the data .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 162,
				"end": 163,
				"text": "k"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_47": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_47",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_47",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_47",
		"text": "\\cref{fig:ar,fig:pd,fig:cl,fig:ir} and \\cref{tbl:basecoef} suggest that our defined problems dimensions all have an impact on experimental results . The reported $\\hat{R^2}$ of our mixed models in \\cref{sec:basic} suggest that choice of motherset , choice of algorithm and our proposed problem dimensions are capable of predicting experimental results with good accuracy . Based on this we are able to recommend using our methodology ( or something appropriately similar ) for controlling and measuring these problem dimensions . We encourage further work that focuses on specific contexts that can be defined by these problem dimensions , especially if it maps these contexts to real - world applications .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 163,
				"end": 172,
				"text": "\\hat{R^2}"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_46": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_46",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_46",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_46",
		"text": "\\begin{table} \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.63\\text width } \\tbl{Mean Performance by Algorithm and Metric (Normalized by a Trivial Solution)\\label{tbl:triv} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean [unused10] } & \\textbf{Mean [unused10] } \\\\ \\hline \\hline abod & 0.0654 & 0.1193 \\\\ egmm & 0.0774 & 0.1265 \\\\ iforest & \\textbf{0.1006} & [unused10] \\\\ loda & 0.0578 & 0.1340 \\\\ lof & 0.0723 & 0.1513 \\\\ ocsvm & - 0.1004 & 0.0792 \\\\ rkde & 0.0707 & 0.1316 \\\\ svdd & - 0.1817 & - 0.5010 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_52": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_52",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_52",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_52",
		"text": "We employed the \\texttt{R} package \\texttt{Rlof} available at \\url{http://cran.open-source-solution.org/web/packages/Rlof/} . We chose $k$ to be 3 \\ % of the dataset . This was the smallest value for which LOF would reliably run on all datasets .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 136,
				"end": 137,
				"text": "k"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_44": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_44",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_44",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_44",
		"text": "By eliminating benchmarks based on hypothesis testing we attempted to account for benchmarks that are too difficult or otherwise unrealistic in composition , but an additional concern of ours is that some benchmarks might be trivially easy . It is often the case in anomaly detection literature that reported results are highly accurate , such as in \\cite{loda,inne,Kriegel:2009,rajasegarar2010centered,Amer:2013} whereas the mean accuracy on our corpus of benchmarks is much lower . In particular , consider the work presented in \\cite{inne} ; the authors share the parameterization of each algorithm on each benchmark and praise the algorithm \\textbf{iNNe} for sometimes performing well with parameter $\\psi=2$ . However , an understanding of the iNNe algorithm and the implication of that parameter choice will reveal that the algorithm is doing little more than approximating the distance of each point from the mean of the data . While we acknowledge that this particular work is a smaller workshop publication , it should serve as a warning that benchmarks for which all algorithms perform well might be benchmarks that can be trivially solved and their inclusion in reported results might not be helpful .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 705,
				"end": 709,
				"text": "\\psi"
			},
			"T2": {
				"eid": "T2",
				"label": "PRIMARY",
				"start": 694,
				"end": 703,
				"text": "parameter"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T2",
				"arg1": "T1",
				"rid": "R1"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_50": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_50",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_50",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_50",
		"text": "To reduce the computational cost of fitting , and to improve the numerical stability of the process , we first transformed each benchmark dataset via principle component analysis . We selected principle components ( in descending eigenvalue order ) to retain 95 \\ % of the variance . To generate the members of the ensemble , we varied the number of clusters $k$ by trying all values in $\\{1,2,3,4, 5, 6\\}$ . For each value of $k$ , we generated 15 GMMs by training on 15 bootstrap replicates of the data and by randomly initializing each replicate . We then computed the average out - of - bag log likelihood for each value of $k$ and discarded $k$ values whose average log likelihood was less than 0.85 times the average log likelihood of the best value of $k$ . The purpose of this was to discard GMMs that do not fit the data very well . Finally , an anomaly score is computed for each point $x$ by computing the average `` surprise ' ' , which is the average negative log probability density $\\frac{1}{L} \\sum_{\\ell=1}^L -\\log P_{\\ell}(x)$ , where $L$ is the number of fitted GMMs and $P_{\\ell}(x)$ is the density assigned by GMM $\\ell$ to data point $x$ . We found in preliminary experiments that this worked better than using the mean probability density $\\frac{1}{L} \\sum_{\\ell=1}^L P_{\\ell}(x)$ .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 360,
				"end": 361,
				"text": "k"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 428,
				"end": 429,
				"text": "k"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 629,
				"end": 630,
				"text": "k"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 647,
				"end": 648,
				"text": "k"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 760,
				"end": 761,
				"text": "k"
			},
			"T6": {
				"eid": "T6",
				"label": "SYMBOL",
				"start": 897,
				"end": 898,
				"text": "x"
			},
			"T7": {
				"eid": "T7",
				"label": "SYMBOL",
				"start": 998,
				"end": 1009,
				"text": "\\frac{1}{L}"
			},
			"T8": {
				"eid": "T8",
				"label": "SYMBOL",
				"start": 1007,
				"end": 1008,
				"text": "L"
			},
			"T9": {
				"eid": "T9",
				"label": "SYMBOL",
				"start": 1016,
				"end": 1020,
				"text": "\\ell"
			},
			"T10": {
				"eid": "T10",
				"label": "SYMBOL",
				"start": 1024,
				"end": 1025,
				"text": "L"
			},
			"T11": {
				"eid": "T11",
				"label": "SYMBOL",
				"start": 1032,
				"end": 1043,
				"text": "P_{\\ell}(x)"
			},
			"T12": {
				"eid": "T12",
				"label": "SYMBOL",
				"start": 1054,
				"end": 1055,
				"text": "L"
			},
			"T13": {
				"eid": "T13",
				"label": "SYMBOL",
				"start": 1091,
				"end": 1102,
				"text": "P_{\\ell}(x)"
			},
			"T14": {
				"eid": "T14",
				"label": "SYMBOL",
				"start": 1136,
				"end": 1140,
				"text": "\\ell"
			},
			"T15": {
				"eid": "T15",
				"label": "SYMBOL",
				"start": 1157,
				"end": 1158,
				"text": "x"
			},
			"T16": {
				"eid": "T16",
				"label": "SYMBOL",
				"start": 1263,
				"end": 1274,
				"text": "\\frac{1}{L}"
			},
			"T17": {
				"eid": "T17",
				"label": "SYMBOL",
				"start": 1272,
				"end": 1273,
				"text": "L"
			},
			"T18": {
				"eid": "T18",
				"label": "SYMBOL",
				"start": 1281,
				"end": 1285,
				"text": "\\ell"
			},
			"T19": {
				"eid": "T19",
				"label": "SYMBOL",
				"start": 1289,
				"end": 1290,
				"text": "L"
			},
			"T20": {
				"eid": "T20",
				"label": "SYMBOL",
				"start": 1291,
				"end": 1302,
				"text": "P_{\\ell}(x)"
			},
			"T21": {
				"eid": "T21",
				"label": "PRIMARY",
				"start": 340,
				"end": 358,
				"text": "number of clusters"
			},
			"T22": {
				"eid": "T22",
				"label": "PRIMARY",
				"start": 350,
				"end": 358,
				"text": "clusters"
			},
			"T23": {
				"eid": "T23",
				"label": "PRIMARY",
				"start": 956,
				"end": 996,
				"text": "average negative log probability density"
			},
			"T24": {
				"eid": "T24",
				"label": "PRIMARY",
				"start": 1064,
				"end": 1085,
				"text": "number of fitted GMMs"
			},
			"T25": {
				"eid": "T25",
				"label": "PRIMARY",
				"start": 1111,
				"end": 1159,
				"text": "density assigned by GMM $\\ell$ to data point $x$"
			},
			"T26": {
				"eid": "T26",
				"label": "PRIMARY",
				"start": 1145,
				"end": 1155,
				"text": "data point"
			},
			"T27": {
				"eid": "T27",
				"label": "PRIMARY",
				"start": 1237,
				"end": 1261,
				"text": "mean probability density"
			}
		},
		"relation": {
			"R13": {
				"label": "Count",
				"arg0": "T23",
				"arg1": "T7",
				"rid": "R13"
			},
			"R28": {
				"label": "Direct",
				"arg0": "T25",
				"arg1": "T13",
				"rid": "R28"
			},
			"R30": {
				"label": "Direct",
				"arg0": "T26",
				"arg1": "T15",
				"rid": "R30"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T2",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T3",
				"rid": "R501"
			},
			"R502": {
				"label": "Corefer-Symbol",
				"arg0": "T3",
				"arg1": "T4",
				"rid": "R502"
			},
			"R503": {
				"label": "Corefer-Symbol",
				"arg0": "T4",
				"arg1": "T5",
				"rid": "R503"
			},
			"R504": {
				"label": "Corefer-Symbol",
				"arg0": "T6",
				"arg1": "T15",
				"rid": "R504"
			},
			"R505": {
				"label": "Corefer-Symbol",
				"arg0": "T7",
				"arg1": "T16",
				"rid": "R505"
			},
			"R506": {
				"label": "Corefer-Symbol",
				"arg0": "T8",
				"arg1": "T10",
				"rid": "R506"
			},
			"R507": {
				"label": "Corefer-Symbol",
				"arg0": "T10",
				"arg1": "T12",
				"rid": "R507"
			},
			"R508": {
				"label": "Corefer-Symbol",
				"arg0": "T12",
				"arg1": "T17",
				"rid": "R508"
			},
			"R509": {
				"label": "Corefer-Symbol",
				"arg0": "T17",
				"arg1": "T19",
				"rid": "R509"
			},
			"R5010": {
				"label": "Corefer-Symbol",
				"arg0": "T9",
				"arg1": "T14",
				"rid": "R5010"
			},
			"R5011": {
				"label": "Corefer-Symbol",
				"arg0": "T14",
				"arg1": "T18",
				"rid": "R5011"
			},
			"R5012": {
				"label": "Corefer-Symbol",
				"arg0": "T11",
				"arg1": "T13",
				"rid": "R5012"
			},
			"R5013": {
				"label": "Corefer-Symbol",
				"arg0": "T13",
				"arg1": "T20",
				"rid": "R5013"
			},
			"R5014": {
				"label": "Corefer-Description",
				"arg0": "T23",
				"arg1": "T27",
				"rid": "R5014"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_51": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_51",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_51",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_51",
		"text": "We employed the \\texttt{libsvm} implementation of Chang and Lin \\cite{chang:2011} available at \\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/} . For each benchmark , we employed a Gaussian radial basis function kernel . Selection of kernel bandwidth was done using the DFN method proposed in \\cite{xiao2014} , while the parameter $\\nu$ was set to 0.03 .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 331,
				"end": 334,
				"text": "\\nu"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_45": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_45",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_45",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_45",
		"text": "To investigate this phenomenon further , we ran a trivial algorithm against our corpus of benchmarks . The algorithm simply computes the arithmetic mean of the data and assigns an outlier score to each point based on its euclidean distance from that mean . We use the performance of this algorithm to normalize the performance of the other non-trivial algorithms . For both AUC and AP we compute the ratio of a given algorithm 's performance over the performance of the trivial algorithm . As with logit $(AUC)$ and $\\log(LIFT)$ we then take the natural log of these ratios . $$\\log\\left(\\frac{AUC_{\\text{non-trivial}}}{AUC_{\\text{trivial}}}\\right), \\log\\left(\\frac{AP_{\\text{non-trivial}}}{AP_{\\text{trivial}}}\\right) $$ In the case of AP , this is very similar to our previous transformed metric , except instead of normalizing against random expectation , we are normalizing against the performance of a trivial solution . Under this metric , achieving a perfect score on a benchmark that is also perfectly or almost - perfectly solved by a trivial solution is not given much merit , while a lower score that is a significant improvement on trivial performance is given much more credit . \\cref{tbl:triv} shows the mean performance of each algorithm under these new metrics .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 506,
				"end": 509,
				"text": "AUC"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 517,
				"end": 527,
				"text": "\\log(LIFT)"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 578,
				"end": 648,
				"text": "\\log\\left(\\frac{AUC_{\\text{non-trivial}}}{AUC_{\\text{trivial}}}\\right)"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 650,
				"end": 718,
				"text": "\\log\\left(\\frac{AP_{\\text{non-trivial}}}{AP_{\\text{trivial}}}\\right)"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 498,
				"end": 503,
				"text": "logit"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 567,
				"end": 573,
				"text": "ratios"
			}
		},
		"relation": {
			"R1": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T1",
				"rid": "R1"
			},
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R2"
			},
			"R3": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T2",
				"rid": "R3"
			},
			"R5": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T3",
				"rid": "R5"
			},
			"R6": {
				"label": "Direct",
				"arg0": "T6",
				"arg1": "T4",
				"rid": "R6"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T3",
				"arg1": "T4",
				"rid": "R500"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_41": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_41",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_41",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_41",
		"text": "\\begin{table} \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric with Many Irrelevant Features\\label{tbl:hiir} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean [unused10] } & \\textbf{Mean [unused10] } \\\\ \\hline \\hline abod & 0.4424 & 0.4612 \\\\ eg mm & 0.4561 & 0.5069 \\\\ iforest & \\textbf{0.8117} & [unused10] \\\\ loda & 0.5803 & 0.7722 \\\\ lof & 0.6199 & 0.7187 \\\\ ocsvm & 0.5752 & 0.8527 \\\\ rkde & 0.5477 & 0.6716 \\\\ svdd & 0.3366 & 0.3580 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 385,
				"end": 391,
				"text": "\\ loda"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 489,
				"end": 495,
				"text": "\\ svdd"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_55": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_55",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_55",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_55",
		"text": "The AUC or AP of a random ranking can be seen as a discrete parametric distribution with parameters $n_{\\text{norm}}$ ( number of normals ) and $n_{\\text{anom}}$ ( number of anomalies ) . The distribution is parametric because there are `` only ' ' $(n_{\\text{norm}}+n_{\\text{anom}})!$ possible rankings , meaning there are a finite number of possible AUC or AP scores for a given set of parameters .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 101,
				"end": 116,
				"text": "n_{\\text{norm}}"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 145,
				"end": 160,
				"text": "n_{\\text{anom}}"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 251,
				"end": 266,
				"text": "n_{\\text{norm}}"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 267,
				"end": 282,
				"text": "n_{\\text{anom}}"
			},
			"T5": {
				"eid": "T5",
				"label": "PRIMARY",
				"start": 89,
				"end": 99,
				"text": "parameters"
			}
		},
		"relation": {
			"R2": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T1",
				"rid": "R2"
			},
			"R4": {
				"label": "Direct",
				"arg0": "T5",
				"arg1": "T2",
				"rid": "R4"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T1",
				"arg1": "T3",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T4",
				"rid": "R501"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_54": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_54",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_54",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_54",
		"text": "We implemented the algorithm as suggest by the authors of \\cite{loda} . Each projection used approximately $\\sqrt{d}$ features and a total of $3d$ projections were used , where $d$ is the number of features in the benchmark .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 108,
				"end": 116,
				"text": "\\sqrt{d}"
			},
			"T2": {
				"eid": "T2",
				"label": "SYMBOL",
				"start": 114,
				"end": 115,
				"text": "d"
			},
			"T3": {
				"eid": "T3",
				"label": "SYMBOL",
				"start": 143,
				"end": 145,
				"text": "3d"
			},
			"T4": {
				"eid": "T4",
				"label": "SYMBOL",
				"start": 144,
				"end": 145,
				"text": "d"
			},
			"T5": {
				"eid": "T5",
				"label": "SYMBOL",
				"start": 178,
				"end": 179,
				"text": "d"
			},
			"T6": {
				"eid": "T6",
				"label": "PRIMARY",
				"start": 147,
				"end": 158,
				"text": "projections"
			},
			"T7": {
				"eid": "T7",
				"label": "PRIMARY",
				"start": 184,
				"end": 223,
				"text": "the number of features in the benchmark"
			}
		},
		"relation": {
			"R3": {
				"label": "Count",
				"arg0": "T6",
				"arg1": "T3",
				"rid": "R3"
			},
			"R5": {
				"label": "Count",
				"arg0": "T6",
				"arg1": "T4",
				"rid": "R5"
			},
			"R6": {
				"label": "Direct",
				"arg0": "T7",
				"arg1": "T5",
				"rid": "R6"
			},
			"R500": {
				"label": "Corefer-Symbol",
				"arg0": "T2",
				"arg1": "T4",
				"rid": "R500"
			},
			"R501": {
				"label": "Corefer-Symbol",
				"arg0": "T4",
				"arg1": "T5",
				"rid": "R501"
			}
		}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_40": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_40",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_40",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_40",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric with No Irrelevant Features\\label{tbl:noir} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean [unused10] } & \\textbf{Mean [unused10] } \\\\ \\hline \\hline abod & 1.3706 & [unused10] \\\\ egmm & 1.3522 & 1.2120 \\\\ iforest & 1.3145 & 1.1690 \\\\ loda & 1.1020 & 1.0235 \\\\ lof & 1.1800 & 1.0958 \\\\ ocsvm & 0.5654 & 0.9077 \\\\ rkde & \\textbf{1.4256} & 1.1747 \\\\ svdd & - 0.0118 & 0.2290 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_56": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_56",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_56",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_56",
		"text": "In both cases it is possible to enumerate these scores and compute how much probability mass each score carries , and thus quantiles of these distributions can be computed . However for larger values of $n$ this becomes computationally inefficient .",
		"entity": {
			"T1": {
				"eid": "T1",
				"label": "SYMBOL",
				"start": 204,
				"end": 205,
				"text": "n"
			}
		},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_42": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_42",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_42",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_42",
		"text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric When Clusteredness is Greater Than 0.25\\label{tbl:hicl} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean [unused10] } & \\textbf{Mean [unused10] } \\\\ \\hline \\hline abod & \\textbf{1.0305} & [unused10] \\\\ egmm & 0.7199 & 0.4834 \\\\ iforest & 0.7405 & 0.5075 \\\\ loda & 0.5689 & 0.4225 \\\\ lof & 0.7623 & 0.5336 \\\\ ocsvm & 0.0385 & 0.2316 \\\\ rkde & 0.7179 & 0.5387 \\\\ svdd & 0.0327 & 0.0639 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	},
	"1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_43": {
		"id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_43",
		"phase": "test",
		"topic": "cs.ai",
		"document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
		"paragraph": "paragraph_43",
		"prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_43",
		"text": "\\begin{table} \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric with Mothersets Selected for Dubious Reasons\\label{tbl:nefarious} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean [unused10] } & \\textbf{Mean [unused10] } \\\\ \\hline \\hline abod & 0.6094 & 0.6071 \\\\ eg mm & 0.5826 & 0.6285 \\\\ iforest & 0.7675 & 0.8903\\\\ loda & 0.7981 & [unused10] \\\\ lof & 0.5999 & 0.7340\\\\ ocsvm & 0.5121 & 0.8080 \\\\ rkde & \\textbf{0.8554} & 0.9222\\\\ svdd & 0.1764 & 0.3164 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}",
		"entity": {},
		"relation": {}
	}
}